{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e2bdfc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1b3a5ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_categorical_arr: all_categorical_arr.shape=(100, 10)\n",
      "all_numerical_arr: all_numerical_arr.shape=(100, 10)\n"
     ]
    }
   ],
   "source": [
    "#random gen dataset\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "x_all_categorical_arr = np.random.randint(0, 2, (100, 10))\n",
    "x_all_numerical_arr = np.random.rand(100, 10)\n",
    "y_categorical_arr = np.random.randint(0, 2, 100)\n",
    "#balanced_arr = np.concatenate([balanced_categorical_arr, balanced_numerical_arr], axis=1)\n",
    "print(f\"all_categorical_arr: {all_categorical_arr.shape=}\")\n",
    "print(f\"all_numerical_arr: {all_numerical_arr.shape=}\")\n",
    "#print(f\"balanced_arr: {balanced_arr.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "67df864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = spark.createDataFrame(all_numerical_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3928a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = spark.createDataFrame(y_categorical_arr,['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "51014500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index\n",
    "x_indexed=x_data.withColumn(\"id\",monotonically_increasing_id())\n",
    "y_indexed=y_data.withColumn(\"id\",monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "eef2fea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_1', '_2', '_3', '_4', '_5', '_6', '_7', '_8', '_9', '_10']\n",
      "['y']\n"
     ]
    }
   ],
   "source": [
    "# DEVELOPMENT: create joined df for computation with one id\n",
    "joined_df = x_indexed.join(y_indexed, \"id\").drop('id')\n",
    "x_train = joined_df.drop('y')\n",
    "y_train = joined_df.select('y')\n",
    "print(x_train.columns)\n",
    "print(y_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac62236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap function definition\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# weighted bootstrap subdataset\n",
    "\n",
    "\n",
    "# boostrap data setsplit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a14e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap sampling per tree\n",
    "# create variations of the joined_df baseed on bootstramp algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00374565",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "77d68bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define entropy for classification evaluation crtieria\n",
    "# receives a probably as input to calculate entropy\n",
    "def class_entropy(y_df):\n",
    "\n",
    "    #convert to rdd\n",
    "    y_rdd = y_df.rdd\n",
    "    \n",
    "    #perform count\n",
    "    total_count = y_rdd.count()\n",
    "    \n",
    "    if total_count ==0:\n",
    "        return 0\n",
    "    else:\n",
    "        class_count = y_rdd.map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)\n",
    "        class_p = class_count.map(lambda p: p[1]/total_count)\n",
    "    \n",
    "        #evaluate probability\n",
    "        entropy = class_p.map(lambda p: -p * np.log2(p))\n",
    "    \n",
    "        return entropy.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "f225c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each partition, find the best split given a feature index\n",
    "# x_train & y_train are RDD of x and y variables respectively\n",
    "# feature_index represents the feature being trained\n",
    "# parent_data_count is a global sum broadcasted at the start of computation\n",
    "\n",
    "def find_split(joined_df, feature_index):\n",
    "    \n",
    "    #split x_train & y_train\n",
    "    #x_train = joined_df.drop(joined_df.columns[-1])\n",
    "    y_train = joined_df.select(joined_df.columns[-1])\n",
    "    split_data = joined_df.select(col(joined_df.columns[feature_index]).alias(\"feature\"),col(joined_df.columns[-1]).alias(\"y\"))\n",
    "    \n",
    "    #init variables\n",
    "    parent_entropy = class_entropy(y_train)\n",
    "    parent_data_count = split_data.count() \n",
    "    schema = StructType([\n",
    "        StructField(\"feature\", IntegerType(), True),\n",
    "        StructField(\"index\", IntegerType(), True),\n",
    "        StructField(\"split_value\", FloatType(), True),\n",
    "        StructField(\"info_gain\", FloatType(), True),\n",
    "    ])\n",
    "    best_IG = 0\n",
    "    best_split = 0\n",
    "    best_idx = 0\n",
    "    \n",
    "    \n",
    "    #for each point in x_train for feature_index, compute information gain\n",
    "    for (idx,split_col) in enumerate(split_data.rdd.collect()): \n",
    "        \n",
    "        #get split value\n",
    "        split_value = split_col[0]\n",
    "        \n",
    "        #threshold\n",
    "        split_data = split_data.withColumn(\"x_child_left\", when(split_data[0] <= split_value,True).otherwise(False))\n",
    "        split_data = split_data.withColumn(\"x_child_right\", when(split_data[0] > split_value,True).otherwise(False))\n",
    "        \n",
    "        #join and get\n",
    "        #joined_df = x_threshold.join(y_train,col('index'))\n",
    "        y_child_left = split_data.filter(col(\"x_child_left\")).select(col('y'))\n",
    "        y_child_right = split_data.filter(col(\"x_child_right\")).select(col('y'))\n",
    "    \n",
    "        #calculate entropy\n",
    "        entropy_left = class_entropy(y_child_left)\n",
    "        entropy_right = class_entropy(y_child_right)\n",
    "        \n",
    "        #calculate Information Gain\n",
    "        num_left, num_right = y_child_left.count(), y_child_right.count()\n",
    "        if num_left != 0 and num_right != 0:\n",
    "            left_weighted = entropy_left * (num_left / parent_data_count)\n",
    "            right_weighted = entropy_right * (num_right/parent_data_count)\n",
    "            IG =  float(parent_entropy  - left_weighted - right_weighted)\n",
    "\n",
    "            #check best\n",
    "            if IG > best_IG:\n",
    "                best_IG = IG\n",
    "                best_split = split_value\n",
    "                best_idx = idx\n",
    "    \n",
    "    IG_df = spark.createDataFrame([(feature_index,best_idx,float(best_split),float(best_IG))], schema)\n",
    "        \n",
    "    return IG_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "a865cd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#test\n",
    "a = find_split(joined_df, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "bd67202f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------+-----------+\n",
      "|feature|index|split_value|  info_gain|\n",
      "+-------+-----+-----------+-----------+\n",
      "|      4|   48|  0.3609739|0.015505971|\n",
      "+-------+-----+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define node class\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    The Node class represents the node information from the tree\n",
    "\n",
    "    Attributes:\n",
    "        criteria (string): the split criterioa used \"gini\" or \"info_gain\"\n",
    "        gini (float): The Gini impurity of the node if available\n",
    "        info_gain (float): The information gain of the node if available\n",
    "        num_data (int): The total number of samples in the node.\n",
    "        num_samples_per_feature (list): The number of samples per class in the node.\n",
    "        prediction (int): prediction of the node\n",
    "        feature_index (int): The index of the feature used for splitting the node.\n",
    "        split_value (float): The splitting value used for the feature_index\n",
    "        left (Node): The left child node.\n",
    "        right (Node): The right child node.\n",
    "        partition (int): The partition number the Node is currently operating on\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, criteria, num_data, num_samples_per_feature, predicted_class):\n",
    "        self.criteria = criteria\n",
    "        self.num_data = num_data\n",
    "        self.num_samples_per_feature = num_samples_per_feature\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.split_value = 0\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.partition = None\n",
    "        \n",
    "    def set_gini(self, gini):\n",
    "        self.criteria = \"gini\"\n",
    "        self.gini = gini\n",
    "    \n",
    "    def set_info_gain(self,info_gain):\n",
    "        self.criteria = \"info_gain\"\n",
    "        self.info_gain = info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4bf2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo to build tree \n",
    "NUM_FEATURES = 10\n",
    "\n",
    "# maintain of list of node - Parent & Child nodes\n",
    "\n",
    "# each tree\n",
    "# (i) for each feature: find_split\n",
    "# (ii) Mapbypartition(find_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e852ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e4153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53acfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c999f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
