{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2bdfc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deb03d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "1b3a5ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_categorical_arr: x_all_categorical_arr.shape=(100, 10)\n",
      "all_numerical_arr: x_all_numerical_arr.shape=(100, 10)\n"
     ]
    }
   ],
   "source": [
    "#random gen dataset\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "x_all_categorical_arr = np.random.randint(0, 2, (100, 10))\n",
    "x_all_numerical_arr = np.random.rand(100, 10)\n",
    "y_categorical_arr = np.random.randint(0, 2, 100)\n",
    "#balanced_arr = np.concatenate([balanced_categorical_arr, balanced_numerical_arr], axis=1)\n",
    "print(f\"all_categorical_arr: {x_all_categorical_arr.shape=}\")\n",
    "print(f\"all_numerical_arr: {x_all_numerical_arr.shape=}\")\n",
    "#print(f\"balanced_arr: {balanced_arr.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "67df864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = spark.createDataFrame(x_all_numerical_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "3928a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = spark.createDataFrame(y_categorical_arr,['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "51014500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index\n",
    "x_indexed=x_data.withColumn(\"id\",monotonically_increasing_id())\n",
    "y_indexed=y_data.withColumn(\"id\",monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "eef2fea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_1', '_2', '_3', '_4', '_5', '_6', '_7', '_8', '_9', '_10']\n",
      "['y']\n"
     ]
    }
   ],
   "source": [
    "# DEVELOPMENT: create joined df for computation with one id\n",
    "joined_df = x_indexed.join(y_indexed, \"id\").drop('id')\n",
    "x_train = joined_df.drop('y')\n",
    "y_train = joined_df.select('y')\n",
    "print(x_train.columns)\n",
    "print(y_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72908b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "4ac62236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bootstrap function definition\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# weighted bootstrap subdataset\n",
    "\n",
    "#partition the dataframe dataset\n",
    "joined_df.repartition(10).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470df42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "06a14e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap sampling per tree\n",
    "# create variations of the joined_df baseed on bootstramp algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "28f35d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boostrap data setsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00374565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "98e8d937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997114417528099"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_entropy(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "77d68bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define entropy for classification evaluation crtieria\n",
    "# receives a probably as input to calculate entropy\n",
    "\n",
    "\n",
    "def class_entropy(df):\n",
    "    # Example entropy calculation for binary classification\n",
    "    col_name = \"y\"\n",
    "    counts = df.groupBy(col_name).count()\n",
    "    total = df.count()\n",
    "    return counts.withColumn(\"prob\", F.col(\"count\") / total).select(\n",
    "        F.sum(-F.col(\"prob\") * F.log2(F.col(\"prob\"))).alias(\"entropy\")\n",
    "    ).first()[\"entropy\"]\n",
    "\n",
    "def prob(df):\n",
    "    # Example entropy calculation for binary classification\n",
    "    count = np.count_nonzero(df)   \n",
    "    if count == 0:\n",
    "        return float(0)\n",
    "    else:\n",
    "        total = len(df)\n",
    "        prob = np.divide(count,total)\n",
    "        return float(prob)\n",
    "\n",
    "class_entropy_udf = udf(class_entropy, ArrayType(DoubleType()))\n",
    "prob_udf = udf(prob,FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a06a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee16bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "5fd7d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_split(joined_df, feature_index):\n",
    "    \n",
    "    # Select relevant columns\n",
    "    feature_col_name = joined_df.columns[feature_index]\n",
    "    y_col_name = joined_df.columns[-1]\n",
    "    split_data = joined_df.select(feature_col_name, y_col_name)\\\n",
    "        .withColumnRenamed(feature_col_name,\"split_value\")\\\n",
    "        .withColumnRenamed(y_col_name,\"y\")\n",
    "    \n",
    "    # Calculate parent entropy\n",
    "    parent_entropy = class_entropy(joined_df.select(\"y\"))\n",
    "    parent_data_count = joined_df.count()\n",
    "    \n",
    "    # Calculate potential splits and their Information Gain\n",
    "    distinct_values = split_data.select(\"split_value\")\\\n",
    "        .withColumnRenamed(\"split_value\",\"feature\")\\\n",
    "        .distinct().orderBy(\"feature\")\n",
    "    \n",
    "    # Cartesian join to get split mask\n",
    "    splits_info = distinct_values.crossJoin(split_data)\\\n",
    "        .withColumn(\n",
    "        \"is_left\", F.col(\"feature\") <= F.col(\"split_value\")\n",
    "    )\n",
    "    \n",
    "    #aggregate list\n",
    "    entropies = splits_info.groupBy(\"split_value\", \"is_left\").agg(\n",
    "        F.count(\"y\").alias(\"count\"),\n",
    "        F.sum(\"y\").alias(\"sum\"),\n",
    "        prob_udf(F.collect_list(\"y\")).alias(\"prob\")\n",
    "    )\n",
    "    entropies = entropies.withColumn(\"entropy\",\\\n",
    "                                    -F.col(\"prob\") * F.log2(F.col(\"prob\")) \\\n",
    "                                    -(1-F.col(\"prob\")) * F.log2((1-F.col(\"prob\")))\n",
    "                                    )\n",
    "    # Calculate Information Gain for each split\n",
    "    info_gain = entropies.groupBy(\"split_value\").agg(\n",
    "        (parent_entropy - F.sum(F.col(\"entropy\") * (F.col(\"count\") / parent_data_count))).alias(\"info_gain\")\n",
    "    )\n",
    "    \n",
    "    # Get the best split\n",
    "    best_split = info_gain.orderBy(F.desc(\"info_gain\")).first()\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"feature\", IntegerType(), True),\n",
    "        StructField(\"split_value\", FloatType(), True),\n",
    "        StructField(\"info_gain\", FloatType(), True),\n",
    "    ])\n",
    "    \n",
    "    # Prepare output DataFrame\n",
    "    result_df = spark.createDataFrame([(feature_index, float(best_split[\"split_value\"]), best_split[\"info_gain\"])], schema)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "1fb0420b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+\n",
      "|feature|split_value|info_gain|\n",
      "+-------+-----------+---------+\n",
      "|      0| 0.46267977|     NULL|\n",
      "+-------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = new_split(joined_df,0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "e6e11fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tree from splitting\n",
    "\n",
    "# each tree\n",
    "# (i) for each feature: find_split\n",
    "# (ii) Mapbypartition(find_split)\n",
    "\n",
    "def feature_split(dataset, feature_array):\n",
    "    \n",
    "    ''' \n",
    "    Input: \n",
    "    partition: a pyspark dataframe partition to be called by foreachPartition,\n",
    "    feature_array: a broadcasted feature array for the tree that is intiialized earlier on\n",
    "    '''\n",
    "    #define schema\n",
    "    schema = StructType([\n",
    "        StructField(\"feature\", IntegerType(), True),\n",
    "        StructField(\"split_value\", FloatType(), True),\n",
    "        StructField(\"info_gain\", FloatType(), True),\n",
    "    ])\n",
    "    feature_df = spark.createDataFrame([], schema)\n",
    "    \n",
    "    # for each feature array, get a split and append the dataframe \n",
    "    for feature_index in feature_array:\n",
    "        \n",
    "        # find split\n",
    "        feature_split = new_split(dataset, feature_index)\n",
    "        \n",
    "        #add feature  \n",
    "        feature_df = feature_df.union(feature_split)\n",
    "        \n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "f225c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = feature_split(joined_df,[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "38d608e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(feature=0, split_value=0.46267977356910706, info_gain=None)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = test.collect()\n",
    "test1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "a865cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_tree(df,feature_array, depth=0, max_depth=3):\n",
    "    \n",
    "    y_label = df.columns[-1]\n",
    "    \n",
    "    if depth == max_depth or class_entropy(df.select(y_label)) == 0:\n",
    "        # Return the most common label\n",
    "        return df.groupBy(y_label).count().orderBy(y_label, ascending=False).first()['label']\n",
    "    \n",
    "    best_feature = None\n",
    "    best_gain = 0\n",
    "    \n",
    "    #get features\n",
    "    feature_df = feature_split(df,feature_array)\n",
    "    feature_list = feature_df.collect()\n",
    "    \n",
    "    for feature in feature_list:  # Assume feature columns\n",
    "        gain = feature[2]\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_feature = feature[0]\n",
    "\n",
    "    if best_feature is None:\n",
    "        return df.groupBy(y_label).count().orderBy('count', ascending=False).first()[y_label]\n",
    "\n",
    "    \n",
    "    # Recursive split\n",
    "    left_df = df.filter(col(best_feature) <= 0.5)  # Assume binary split\n",
    "    right_df = df.filter(col(best_feature) > 0.5)\n",
    "    \n",
    "    left_tree = grow_tree(left_df, depth + 1, max_depth)\n",
    "    right_tree = grow_tree(right_df, depth + 1, max_depth)\n",
    "    return {best_feature: {'left': left_tree, 'right': right_tree}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "f1fb1531",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[409], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m grow_tree(joined_df,[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m],\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[408], line 18\u001b[0m, in \u001b[0;36mgrow_tree\u001b[0;34m(df, feature_array, depth, max_depth)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m feature_list:  \u001b[38;5;66;03m# Assume feature columns\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     gain \u001b[38;5;241m=\u001b[39m feature[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gain \u001b[38;5;241m>\u001b[39m best_gain:\n\u001b[1;32m     19\u001b[0m         best_gain \u001b[38;5;241m=\u001b[39m gain\n\u001b[1;32m     20\u001b[0m         best_feature \u001b[38;5;241m=\u001b[39m feature[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "test = grow_tree(joined_df,[0,1,2],1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "3f6d921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_forest_train(df, num_trees, max_depth=3):\n",
    "    trees = []\n",
    "    num_features = len(df.first()['features'])\n",
    "    for _ in range(num_trees):\n",
    "        sampled_df = df.sample(withReplacement=True, fraction=1.0)\n",
    "        feature_indices = random.sample(range(num_features), k=int(log(num_features, 2) + 1))\n",
    "        feature_cols = [f\"features[{i}]\" for i in feature_indices]\n",
    "        tree = grow_tree(sampled_df, 0, max_depth)\n",
    "        trees.append(tree)\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6214602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a35c107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40b17362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "785c7944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4bf2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e852ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e4153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old\n",
    "'''\n",
    "def class_entropy(y_df):\n",
    "\n",
    "    #convert to rdd\n",
    "    y_rdd = y_df.rdd\n",
    "    \n",
    "    #perform count\n",
    "    total_count = y_rdd.count()\n",
    "    \n",
    "    if total_count ==0:\n",
    "        return 0\n",
    "    else:\n",
    "        class_count = y_rdd.map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)\n",
    "        class_p = class_count.map(lambda p: p[1]/total_count)\n",
    "    \n",
    "        #evaluate probability\n",
    "        entropy = class_p.map(lambda p: -p * np.log2(p))\n",
    "    \n",
    "        return entropy.collect()[0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old\n",
    "#for each partition, find the best split given a feature index\n",
    "# x_train & y_train are RDD of x and y variables respectively\n",
    "# feature_index represents the feature being trained\n",
    "# parent_data_count is a global sum broadcasted at the start of computation\n",
    "\n",
    "def find_split(joined_df, feature_index):\n",
    "    \n",
    "    #split x_train & y_train\n",
    "    \n",
    "    y_train = joined_df.select(joined_df.columns[-1])\n",
    "    split_data = joined_df.select(col(joined_df.columns[feature_index]).alias(\"feature\"),col(joined_df.columns[-1]).alias(\"y\"))\n",
    "    \n",
    "    #init variables\n",
    "    parent_entropy = class_entropy(y_train)\n",
    "    parent_data_count = split_data.count() \n",
    "    schema = StructType([\n",
    "        StructField(\"feature\", IntegerType(), True),\n",
    "        StructField(\"index\", IntegerType(), True),\n",
    "        StructField(\"split_value\", FloatType(), True),\n",
    "        StructField(\"info_gain\", FloatType(), True),\n",
    "    ])\n",
    "    best_IG = 0\n",
    "    best_split = 0\n",
    "    best_idx = 0\n",
    "    \n",
    "    \n",
    "    #for each point in x_train for feature_index, compute information gain\n",
    "    for (idx,split_col) in enumerate(split_data.rdd.distinct().collect()): \n",
    "        \n",
    "        #get split value\n",
    "        split_value = split_col[0]\n",
    "        \n",
    "        #threshold\n",
    "        split_data = split_data.withColumn(\"x_child_left\", when(split_data[0] <= split_value,True).otherwise(False))\n",
    "        split_data = split_data.withColumn(\"x_child_right\", when(split_data[0] > split_value,True).otherwise(False))\n",
    "        \n",
    "        #join and get\n",
    "        #joined_df = x_threshold.join(y_train,col('index'))\n",
    "        y_child_left = split_data.filter(col(\"x_child_left\")).select(col('y'))\n",
    "        y_child_right = split_data.filter(col(\"x_child_right\")).select(col('y'))\n",
    "    \n",
    "        #calculate entropy\n",
    "        entropy_left = class_entropy(y_child_left)\n",
    "        entropy_right = class_entropy(y_child_right)\n",
    "        \n",
    "        #calculate Information Gain\n",
    "        num_left, num_right = y_child_left.count(), y_child_right.count()\n",
    "        if num_left != 0 and num_right != 0:\n",
    "            left_weighted = entropy_left * (num_left / parent_data_count)\n",
    "            right_weighted = entropy_right * (num_right/parent_data_count)\n",
    "            IG =  float(parent_entropy  - left_weighted - right_weighted)\n",
    "\n",
    "            #check best\n",
    "            if IG > best_IG:\n",
    "                best_IG = IG\n",
    "                best_split = split_value\n",
    "                best_idx = idx\n",
    "    \n",
    "    IG_df = spark.createDataFrame([(feature_index,best_idx,float(best_split),float(best_IG))], schema)\n",
    "        \n",
    "    return IG_df\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
